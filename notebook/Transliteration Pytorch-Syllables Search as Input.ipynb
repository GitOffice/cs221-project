{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much of this code, particularly the encoder decoder code, is taken from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# the rest is adapted for this project but is still pretty similar\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for start and end of string\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Alphabet class (works with both pinyin and English)\n",
    "class Alphabet:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.letter2index = {}\n",
    "        self.letter2count = {}\n",
    "        self.index2letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2\n",
    "        \n",
    "    def add_name(self, name):\n",
    "        \"\"\"\n",
    "        Adds the characters of a name to the alphabet by iterating over them\n",
    "        and updating the appropriate counts\n",
    "        \"\"\"\n",
    "        # for pinyin we can use the syllables instead of the raw letters. \n",
    "        name = name.split(\" \") # they'll still fall into the \"letter\" category though\n",
    "            \n",
    "        for letter in name: \n",
    "            if letter not in self.letter2index:\n",
    "                self.letter2index[letter] = self.n_letters\n",
    "                self.letter2count[letter] = 1\n",
    "                self.index2letter[self.n_letters] = letter\n",
    "                self.n_letters += 1\n",
    "            else:\n",
    "                self.letter2count[letter] += 1\n",
    "\n",
    "                    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names have: 148 characters\n",
      "Output names have: 279 characters\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(\"..\", \"data\", \"TransliterationSearchAndData.csv\")\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Right now just converts a string to lowercase but could be something more later\n",
    "    (such as removing spaces)\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"([-.Â·])\", r\"\", s) # remove punctuation that seems to have seeped in (including chinese dash)\n",
    "    return s.lower()\n",
    "\n",
    "def read_alphabets():\n",
    "    \"\"\"\n",
    "    Creates two alphabets, one for search output names and the other for ground truth pinyin\n",
    "    Iterates through data file to initialize those alphabets\n",
    "    \"\"\"\n",
    "    input_alph = Alphabet(\"SearchOutput\")\n",
    "    output_alph = Alphabet(\"Pinyin\")\n",
    "    pairs = []\n",
    "    \n",
    "    df = pd.read_csv(data_file)\n",
    "    for row_i, row in df.iterrows():\n",
    "        pinyin, search_output = row\n",
    "        search_output = normalize(search_output)\n",
    "        pinyin = normalize(pinyin)\n",
    "        input_alph.add_name(search_output)\n",
    "        output_alph.add_name(pinyin) # includes spaces\n",
    "        pairs.append([search_output, pinyin])\n",
    "     \n",
    "    print(\"Input names have: {} characters\".format(input_alph.n_letters))\n",
    "    print(\"Output names have: {} characters\".format(output_alph.n_letters))\n",
    "\n",
    "    return input_alph, output_alph, pairs\n",
    "        \n",
    "srch_alph, pin_alph, pairs = read_alphabets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embedding is really just a lookup table that takes an index input and returns some k-dimensional vector\n",
    "        # input_size is size of table, hidden size is number of weights associated with each vector. We really only\n",
    "        # need this to be a one-hot vector for our purposes so we can probably don't need to explicity represent an \n",
    "        # embedding. The smaller our embedding dimension the more information we're giving up\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)#, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # still need an embedding\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # assume because we have 1-d data\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output = self.embedding(x).view(1, 1, -1)\n",
    "        output = F.relu(output) # regularization thing\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0])) # output is only going to have a single thing, so this is legal i guess\n",
    "        return output, hidden\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromName(alphabet, name):\n",
    "    return [alphabet.letter2index[l] for l in name.split(\" \")]\n",
    "\n",
    "def tensorFromName(alphabet, name):\n",
    "    indexes = indexesFromName(alphabet, name)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromName(srch_alph, pair[0])\n",
    "    target_tensor = tensorFromName(pin_alph, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often we use the target input as input to our decoder rather than our decoder's guess\n",
    "# while training \n",
    "teacher_forcing_ratio = 0.5 \n",
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "         criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden() # just 0's\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "    \n",
    "    loss = 0 # mission accomplished ;)\n",
    "    \n",
    "    # actually run the thing that encodes\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    \n",
    "    # now its decoder time - this part changes somewhat if you add attention\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden # no need for an init function\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # target is next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di] # bc we're using teacher focing\n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1) # returns a tuple of the largest value and its index as tensors\n",
    "            decoder_input = topi.squeeze().detach() # I'm not totally sure what this does\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break # we're done with this sentence - we don't have to do this above bc it goes to the end of the string automatically\n",
    "    \n",
    "    \n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # run SGD which is in the encoder/decoder_optimizer object\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()/target_length # not sure what this is, but we can see I guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied directly for profiling...\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually do the training:\n",
    "\n",
    "def trainIters(encoder, decoder, training_pairs, n_iters, print_every=1000, plot_every=100, learning_rate = 0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(training_pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss() # this is... negative log likelihood loss\n",
    "                             # it's the same as cross-entropy loss bc of the log softmax in the last layer\n",
    "    \n",
    "    for iter_i in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_i -1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder,\n",
    "                     encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter_i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every # calc avg loss\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter_i / n_iters),\n",
    "                                         iter_i, iter_i / n_iters * 100, print_loss_avg))\n",
    "        # for plotting loss\n",
    "        if iter_i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#shamelessly copied from tutorial... yikes\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as training, just no targets - just rum the thing through the network\n",
    "\n",
    "def evaluate(encoder, decoder, name, max_len=MAX_LENGTH):\n",
    "    with torch.no_grad(): # not totally sure what this does tbh - probably stops from updating gradients like we do in training because we are done with training\n",
    "        input_tensor = tensorFromName(srch_alph, name)\n",
    "        input_length = input_tensor.size(0) # just the size of the first dimension\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_len, encoder.hidden_size)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0] # is a vector \n",
    "        \n",
    "        # decoder - would have to change if added attention\n",
    "        decoder_input = torch.tensor([[SOS_token]])\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_name = []\n",
    "        \n",
    "        for di in range(max_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            # transliterate to actual words \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_name.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_name.append(pin_alph.index2letter[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            \n",
    "        return decoded_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomLines(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_name = evaluate(encoder, decoder, pair[0])\n",
    "        print('<', ''.join(output_name[:-1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAllLines(encoder, decoder, eval_pairs, spaces = False):\n",
    "    distance = 0\n",
    "    diff_count = 0\n",
    "    for pair in eval_pairs:\n",
    "        output_name = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        # maintain spaces\n",
    "        if spaces:\n",
    "            output_name  = ''.join(output_name[:-1])\n",
    "            target_name = pair[1]\n",
    "            print(output_name, target_name)\n",
    "        else:\n",
    "            # remove the space for edit distance calculations for consistency with baseline\n",
    "            output_name = ''.join(filter(lambda l: l != ' ', output_name[:-1])) # need to get rid of the <EOS> string at end\n",
    "            target_name = ''.join(filter(lambda l: l != ' ', pair[1]))\n",
    "        if output_name != target_name:\n",
    "            \n",
    "            diff_count += 1\n",
    "            distance += edit_distance_pinyin(target_name, output_name)\n",
    "    print(\"Out of {} names, {} were different, with an average edit distance of {} ({} for just the different pairs)\".format(len(eval_pairs), diff_count, distance/len(eval_pairs), distance/diff_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(encoder, decoder):\n",
    "    srch_names = [pair[0] for pair in pairs]\n",
    "    pinyin_names = [pair[1] for pair in pairs]\n",
    "    train_srch, test_srch, train_pin, test_pin = train_test_split(srch_names, pinyin_names, test_size=100)\n",
    "    train_pairs = list(zip(train_srch, train_pin))\n",
    "    test_pairs = list(zip(test_srch, test_pin))\n",
    "    #trainIters(encoder, decoder, train_pairs, 75000, print_every=5000)\n",
    "    \n",
    "    evaluateAllLines(encoder, decoder, test_pairs)\n",
    "    return train_pairs, test_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 100 names, 100 were different, with an average edit distance of 53.67 (53.67 for just the different pairs)\n"
     ]
    }
   ],
   "source": [
    "# now actually do the thing!\n",
    "hidden_size = 20\n",
    "encoder = EncoderRNN(srch_alph.n_letters, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, pin_alph.n_letters)\n",
    "\n",
    "train, test = train_and_evaluate(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> bÄ Ã©r Ã¨ lÃ¨\n",
      "= dÃ¡ ruÃ¬ Är\n",
      "< kÄnÃ¡njÄ«kÇnpÄnjÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇn\n",
      "\n",
      "> bÃ¬ bÄ\n",
      "= lÃ¬ wÃ¡\n",
      "< kÄyÃ­nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ nÃ \n",
      "\n",
      "> bÄ cuÃ¬ Ã¨\n",
      "= jiÃ© kÃ¨\n",
      "< kÄnÃ¡njÄ«kÇnpÄnjÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇn\n",
      "\n",
      "> bo sÃ \n",
      "= luÃ³ shÄ\n",
      "< kÄnÃ¡njÄ«kÇnpÄnjÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇn\n",
      "\n",
      "> bo sÄ« Ã¨\n",
      "= luÃ³ xÄ«\n",
      "< kÄyÃ­yuÄjÄ«jÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇnsÃ²ng\n",
      "\n",
      "> bÄn\n",
      "= yÄ« bÄng\n",
      "< kÄyÃ­yuÄjÄ«jÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇnsÃ²ng\n",
      "\n",
      "> de Än\n",
      "= dÃ­ Än\n",
      "< kÄyÃ­yuÄjÄ«jÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇnsÃ²ng\n",
      "\n",
      "> bÄ ruÃ¬ bo\n",
      "= mÇ lÇ Ã o\n",
      "< kÄyÃ­yuÄjÄ«jÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇnsÃ²ng\n",
      "\n",
      "> sÃ i yÃ \n",
      "= dÃ i xÄ«\n",
      "< kÄyÃ­yuÄjÄ«jÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇnsÃ²ng\n",
      "\n",
      "> bÄ bÃ i Ã¨ lÃ¨ lÃ¨\n",
      "= jiÄ bÇ lÇ lÄ\n",
      "< kÄnÃ¡njÄ«kÇnpÄnjÄ«kÇnsÃ²ngkÇnsÃ²ngkÇnsÃ²ngtÄ«ngpÄnjÄ«kÇnpÄnjÄ«kÇn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomLines(encoder, decoder, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wÃ©idÃ¬ \n",
      "Out of 1 names, 1 were different, with an average edit distance of 5.0 (5.0 for just the different pairs)\n"
     ]
    }
   ],
   "source": [
    "evaluateAllLines(encoder2, decoder2, [('zev', '')], spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 100 names, 93 were different, with an average edit distance of 3.54 (3.806451612903226 for just the different pairs)\n"
     ]
    }
   ],
   "source": [
    "# now actually do the thing!\n",
    "hidden_size = 20\n",
    "#encoder2 = EncoderRNN(eng_alph.n_letters, hidden_size)\n",
    "#decoder2 = DecoderRNN(hidden_size, pin_alph.n_letters)\n",
    "\n",
    "train, test = train_and_evaluate(encoder2, decoder2)\n",
    "#trainIters(encoder2, decoder2, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sampson\n",
      "= xÄ«n pÇ sÄn\n",
      "< shÄmÇsÄn\n",
      "\n",
      "> boyd\n",
      "= bÃ¹ dÃ©\n",
      "< bÃ¹dÃ©\n",
      "\n",
      "> bernice\n",
      "= bÇi nÄ« sÄ«\n",
      "< bÃ¹lÃ¡nnÃ­\n",
      "\n",
      "> denise\n",
      "= dÄn nÄ« sÄ«\n",
      "< wÃ©inÃ­sÄ«\n",
      "\n",
      "> martina\n",
      "= mÇ dÃ¬ nÃ \n",
      "< mÇdÃ¬nÃ \n",
      "\n",
      "> maurice\n",
      "= mÃ³ lÇ sÄ«\n",
      "< mÇlÇsÄ«\n",
      "\n",
      "> cathy\n",
      "= kÇi xÄ«\n",
      "< kÇixÄ«\n",
      "\n",
      "> janice\n",
      "= zhÄn nÄ« sÄ«\n",
      "< zhÄnnÄ«sÄ«\n",
      "\n",
      "> flora\n",
      "= fÃº luÅ lÄ\n",
      "< fÃºlÄlÄ\n",
      "\n",
      "> trina\n",
      "= cuÄ« nÃ \n",
      "< tÃ¨tÃ¨nÃ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomLines(encoder2, decoder2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def save_model(encoder, decoder):\n",
    "    torch.save(encoder.state_dict(), os.path.join(\"..\", \"models\", \"{date:%Y-%m-%d-%H:%M:%S}-encoder\".format(date=datetime.datetime.now())))\n",
    "    torch.save(decoder.state_dict(), os.path.join(\"..\", \"models\", \"{date:%Y-%m-%d-%H:%M:%S}-decoder\".format(date=datetime.datetime.now())))\n",
    "\n",
    "save_model(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(encoder_path, decoder_path):\n",
    "    encoder1 = EncoderRNN(srch_alph.n_letters, hidden_size)\n",
    "    decoder1 = DecoderRNN(hidden_size, pin_alph.n_letters)\n",
    "    encoder1.load_state_dict(torch.load(encoder_path))\n",
    "    decoder1.load_state_dict(torch.load(decoder_path))\n",
    "    return encoder1, decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcde'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EOS',\n",
       " 'SOS',\n",
       " 'bo',\n",
       " 'bÃ i',\n",
       " 'bÃ o',\n",
       " 'bÃ¡i',\n",
       " 'bÃ¨i',\n",
       " 'bÃ¬',\n",
       " 'bÃ³',\n",
       " 'bÃ¹',\n",
       " 'bÄ',\n",
       " 'bÄn',\n",
       " 'bÄng',\n",
       " 'bÄn',\n",
       " 'bÄ«n',\n",
       " 'bÅ',\n",
       " 'bÇi',\n",
       " 'bÇo',\n",
       " 'bÇ',\n",
       " 'chÃ¡',\n",
       " 'chÃ¨',\n",
       " 'cuÃ¬',\n",
       " 'cuÄ«',\n",
       " 'cÃ i',\n",
       " 'de',\n",
       " 'diÄn',\n",
       " 'duÅ',\n",
       " 'dÃ ',\n",
       " 'dÃ i',\n",
       " 'dÃ o',\n",
       " 'dÃ¡',\n",
       " 'dÃ¨ng',\n",
       " 'dÃ©',\n",
       " 'dÃ¬',\n",
       " 'dÃ­',\n",
       " 'dÃ¹',\n",
       " 'dÃ¹n',\n",
       " 'dÄn',\n",
       " 'dÄng',\n",
       " 'dÄng',\n",
       " 'dÄ«',\n",
       " 'dÄ«ng',\n",
       " 'dÅng',\n",
       " 'dÅu',\n",
       " 'dÅ«',\n",
       " 'dÅ«n',\n",
       " 'fÃ n',\n",
       " 'fÃ¡n',\n",
       " 'fÃ¨i',\n",
       " 'fÃ¹',\n",
       " 'fÃº',\n",
       " 'fÄng',\n",
       " 'fÄi',\n",
       " 'fÄn',\n",
       " 'fÄi',\n",
       " 'fÅ«',\n",
       " 'fÇ',\n",
       " 'guÄn',\n",
       " 'guÇ',\n",
       " 'gÃ i',\n",
       " 'gÃ¨',\n",
       " 'gÃ©',\n",
       " 'gÄn',\n",
       " 'gÄo',\n",
       " 'gÄ',\n",
       " 'gÄn',\n",
       " 'gÅ«',\n",
       " 'gÇ',\n",
       " 'huÃ¡',\n",
       " 'huÃ²',\n",
       " 'hÃ n',\n",
       " 'hÃ¡n',\n",
       " 'hÃ¡o',\n",
       " 'hÃ¨',\n",
       " 'hÃ©',\n",
       " 'hÃº',\n",
       " 'hÄ',\n",
       " 'hÄng',\n",
       " 'hÅ«',\n",
       " 'hÇi',\n",
       " 'hÇn',\n",
       " 'hÇo',\n",
       " 'jiÃ©',\n",
       " 'jiÄ',\n",
       " 'jiÇ',\n",
       " 'jiÇn',\n",
       " 'jÃ¬',\n",
       " 'jÃ­',\n",
       " 'jÄ«',\n",
       " 'jÄ«n',\n",
       " 'kuÃ­',\n",
       " 'kÃ¨',\n",
       " 'kÃ²u',\n",
       " 'kÃ¹',\n",
       " 'kÄng',\n",
       " 'kÄ',\n",
       " 'kÄ',\n",
       " 'kÄn',\n",
       " 'kÅ«n',\n",
       " 'kÇ',\n",
       " 'kÇi',\n",
       " 'kÇn',\n",
       " 'kÇo',\n",
       " 'liÃ¡n',\n",
       " 'liÃ¡ng',\n",
       " 'liÃ¨',\n",
       " 'luÃ²',\n",
       " 'luÃ³',\n",
       " 'luÅ',\n",
       " 'lÃ i',\n",
       " 'lÃ¡i',\n",
       " 'lÃ¡n',\n",
       " 'lÃ¡o',\n",
       " 'lÃ¨',\n",
       " 'lÃ©i',\n",
       " 'lÃ¬',\n",
       " 'lÃ¬lÃ¹',\n",
       " 'lÃ­n',\n",
       " 'lÃ­ng',\n",
       " 'lÃ³ng',\n",
       " 'lÃ¹',\n",
       " 'lÃº',\n",
       " 'lÃºn',\n",
       " 'lÃ¼Ã¨',\n",
       " 'lÄ',\n",
       " 'lÄi',\n",
       " 'lÄi',\n",
       " 'lÇng',\n",
       " 'lÇ',\n",
       " 'lÇ',\n",
       " 'lÇ',\n",
       " 'miÃ¹',\n",
       " 'mÃ i',\n",
       " 'mÃ n',\n",
       " 'mÃ¨ng',\n",
       " 'mÃ©i',\n",
       " 'mÃ©n',\n",
       " 'mÃ©ng',\n",
       " 'mÃ¬',\n",
       " 'mÃ­ng',\n",
       " 'mÃ²',\n",
       " 'mÃ³',\n",
       " 'mÃ¹',\n",
       " 'mÄi',\n",
       " 'mÄ«',\n",
       " 'mÇ',\n",
       " 'mÇ',\n",
       " 'mÇn',\n",
       " 'mÇ',\n",
       " 'niÃ¨',\n",
       " 'niÇ',\n",
       " 'nuÃ²',\n",
       " 'nÃ ',\n",
       " 'nÃ i',\n",
       " 'nÃ¡n',\n",
       " 'nÃ¨i',\n",
       " 'nÃ©ng',\n",
       " 'nÃ­',\n",
       " 'nÃ­ng',\n",
       " 'nÃ³ng',\n",
       " 'nÄ«',\n",
       " 'nÇi',\n",
       " 'nÇ',\n",
       " 'pÃ ',\n",
       " 'pÃ i',\n",
       " 'pÃ¨i',\n",
       " 'pÃ©i',\n",
       " 'pÃ­',\n",
       " 'pÃ²',\n",
       " 'pÄn',\n",
       " 'pÇ',\n",
       " 'qiÃ n',\n",
       " 'qiÃ¡ng',\n",
       " 'qiÃ¡o',\n",
       " 'qiÃ¨',\n",
       " 'qiÃ³ng',\n",
       " 'qiÃº',\n",
       " 'qÃ¬',\n",
       " 'qÃ­',\n",
       " 'qÃ­n',\n",
       " 'qÇ',\n",
       " 'ruÃ¬',\n",
       " 'ruÃ²',\n",
       " 'rÃ³u',\n",
       " 'shÃ i',\n",
       " 'shÃ o',\n",
       " 'shÃ©n',\n",
       " 'shÃ¬',\n",
       " 'shÄ',\n",
       " 'shÄn',\n",
       " 'shÄng',\n",
       " 'shÅ«',\n",
       " 'shÇ',\n",
       " 'suÇ',\n",
       " 'sÃ ',\n",
       " 'sÃ i',\n",
       " 'sÃ¨',\n",
       " 'sÃ²ng',\n",
       " 'sÄ',\n",
       " 'sÄi',\n",
       " 'sÄng',\n",
       " 'sÄn',\n",
       " 'sÄ«',\n",
       " 'sÅ«',\n",
       " 'tiÄ',\n",
       " 'tuÅ',\n",
       " 'tÃ i',\n",
       " 'tÃ¡ng',\n",
       " 'tÃ¡o',\n",
       " 'tÃ¨',\n",
       " 'tÃ­',\n",
       " 'tÃ­ng',\n",
       " 'tÃ³ng',\n",
       " 'tÃº',\n",
       " 'tÄng',\n",
       " 'tÄ«ng',\n",
       " 'tÇ',\n",
       " 'tÇn',\n",
       " 'wÃ n',\n",
       " 'wÃ ng',\n",
       " 'wÃ¡',\n",
       " 'wÃ¨i',\n",
       " 'wÃ©i',\n",
       " 'wÃ©n',\n",
       " 'wÃ²',\n",
       " 'wÄi',\n",
       " 'wÄn',\n",
       " 'wÄng',\n",
       " 'wÇ',\n",
       " 'wÇ',\n",
       " 'xiÃ ',\n",
       " 'xiÃ o',\n",
       " 'xiÃ¨',\n",
       " 'xiÄn',\n",
       " 'xiÄ',\n",
       " 'xiÅ«',\n",
       " 'xiÇng',\n",
       " 'xuÄ',\n",
       " 'xÃ­',\n",
       " 'xÃ¹n',\n",
       " 'xÄ«',\n",
       " 'xÄ«n',\n",
       " 'yuÄ',\n",
       " 'yÃ ',\n",
       " 'yÃ¨',\n",
       " 'yÃ©',\n",
       " 'yÃ¬',\n",
       " 'yÃ­',\n",
       " 'yÃ³u',\n",
       " 'yÄ«',\n",
       " 'yÄ«ng',\n",
       " 'yÅu',\n",
       " 'yÇ',\n",
       " 'yÇn',\n",
       " 'yÇ',\n",
       " 'zhuÃ ng',\n",
       " 'zhÃ¨',\n",
       " 'zhÃ©',\n",
       " 'zhÃ¬',\n",
       " 'zhÄ',\n",
       " 'zhÄn',\n",
       " 'zhÄn',\n",
       " 'zhÄ«',\n",
       " 'zhÅ«',\n",
       " 'zuÇ',\n",
       " 'zÃ©',\n",
       " 'zÄ«',\n",
       " 'Ã i',\n",
       " 'Ã o',\n",
       " 'Ã¡ng',\n",
       " 'Ã¨',\n",
       " 'Ã©',\n",
       " 'Ã©r',\n",
       " 'Ä',\n",
       " 'Äi',\n",
       " 'Än',\n",
       " 'Än',\n",
       " 'Är',\n",
       " 'Åu']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pin_alph.index2letter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(data_file)\n",
    "max([len(x) for x in df['first name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'contains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-75e4847d1a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ã©'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/benjaminnewman/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'contains'"
     ]
    }
   ],
   "source": [
    "\"../models/decoder-2018-11-14-23:32:47\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifically for pinyin - doesn't penalize wrong tones as much\n",
    "def edit_distance_pinyin(str1, str2):\n",
    "    cache = {}\n",
    "    def recurse(str1, str2):\n",
    "        # base cases\n",
    "        if len(str1) == 0:\n",
    "            return len(str2)\n",
    "        elif len(str2) == 0:\n",
    "            return len(str1)\n",
    "        \n",
    "        if cache.get((str1, str2), -1) != -1:\n",
    "            return cache[(str1, str2)]\n",
    "        # recursive case\n",
    "        if str1[0] == str2[0]:\n",
    "            ed = recurse(str1[1:], str2[1:])\n",
    "            cache[(str1, str2)] = ed\n",
    "            return ed\n",
    "        if have_diff_tones(str1[0], str2[0]):\n",
    "            ed = 0.5 + recurse(str1[1:], str2[1:])\n",
    "            cache[(str1, str2)] = ed\n",
    "            return ed\n",
    "        else:\n",
    "            # min of insert into 1, insert into 2, replace\n",
    "            ed = 1 + min(recurse(str1, str2[1:]), recurse(str1[1:], str2), recurse(str1[1:], str2[1:]))\n",
    "            cache[(str1, str2)] = ed\n",
    "            return ed\n",
    "\n",
    "    return recurse(str1, str2) #- common_chars(str1, str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifically for pinyin \n",
    "def have_diff_tones(v1, v2):\n",
    "    result = False\n",
    "    if v1 in 'ÄÃ¡ÇÃ a' and v2 in 'ÄÃ¡ÇÃ a':\n",
    "        return True\n",
    "    elif v1 in 'ÄÃ©ÄÃ¨e' and v2 in 'ÄÃ©ÄÃ¨e':\n",
    "        return True\n",
    "    elif v1 in 'Ä«Ã­ÇÃ¬i' and v2 in 'Ä«Ã­ÇÃ¬i':\n",
    "        return True\n",
    "    elif v1 in 'ÅÃ³ÇÃ²o' and v2 in 'ÅÃ³ÇÃ²o':\n",
    "        return True\n",
    "    elif v1 in 'Å«ÃºÇÃ¹u' and v2 in 'Å«ÃºÇÃ¹u':\n",
    "        return True\n",
    "    elif v1 in 'ÇÇÇÇÃ¼' and v2 in 'ÇÇÇÇÃ¼':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance_pinyin('ben', 'bÄn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
